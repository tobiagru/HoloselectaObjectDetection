{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ObjectDetectionKerasApplications.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"WqbQnOjBlwIZ","colab_type":"code","colab":{}},"source":["!pip install tensorflow==2.0.0-beta1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rFBmZgrUzteG","colab_type":"code","outputId":"2db922a1-b9d0-4097-e9f7-1112567270b3","executionInfo":{"status":"ok","timestamp":1564774245980,"user_tz":-120,"elapsed":4666,"user":{"displayName":"Tobias Grundmann","photoUrl":"","userId":"08188606845298769954"}},"colab":{"base_uri":"https://localhost:8080/","height":215}},"source":["!pip install pascal-voc-tools"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting pascal-voc-tools\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/39/f3a438c87c3950102119fe247e2980367e474549b10d97b3c8e1de7ea0fc/pascal_voc_tools-0.1.22-py3-none-any.whl (105kB)\n","\u001b[K     |████████████████████████████████| 112kB 3.5MB/s \n","\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from pascal-voc-tools) (3.4.5.20)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pascal-voc-tools) (4.28.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from pascal-voc-tools) (2.10.1)\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python->pascal-voc-tools) (1.16.4)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->pascal-voc-tools) (1.1.1)\n","Installing collected packages: pascal-voc-tools\n","Successfully installed pascal-voc-tools-0.1.22\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1z2IHPhvlkK2","colab_type":"text"},"source":["Get COCO Dataset"]},{"cell_type":"code","metadata":{"id":"GpJDccktljUM","colab_type":"code","colab":{}},"source":["!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MyW-uktC9SEq","colab_type":"code","colab":{}},"source":["!tar -xvf VOCtrainval_11-May-2012.tar"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"59tpRbaH1U3v","colab_type":"code","colab":{}},"source":["import pascal_voc_tools"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c_OFKxI21XXU","colab_type":"code","outputId":"56965e46-bce1-4931-b125-052ac484dd9a","executionInfo":{"status":"error","timestamp":1564774833658,"user_tz":-120,"elapsed":594,"user":{"displayName":"Tobias Grundmann","photoUrl":"","userId":"08188606845298769954"}},"colab":{"base_uri":"https://localhost:8080/","height":203}},"source":["xml_path = 'VOCdevkit/VOC2012/Annotations/2007_000027.xml'\n","parser = pascal_voc_tools."],"execution_count":0,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-2bf5541d3e5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mxml_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'VOCdevkit/VOC2012/Annotations/2007_000027.xml'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpascal_voc_tools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXmlParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxml_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mann_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: __init__() takes 1 positional argument but 2 were given"]}]},{"cell_type":"code","metadata":{"id":"KLYyV_zN85Ht","colab_type":"code","colab":{}},"source":["from pycocotools.coco import COCO\n","from pycocotools.cocoeval import COCOeval"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1QMZ2EO-88yb","colab_type":"code","colab":{}},"source":["annType = ['segm','bbox','keypoints']\n","annType = annType[1]      #specify type here\n","prefix = 'person_keypoints' if annType=='keypoints' else 'instances'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"23pDFvG79Aej","colab_type":"code","colab":{}},"source":["dataDir='../'\n","dataType='val2014'\n","annFile = '%sannotations/%s_%s.json'%(dataDir,prefix,dataType)\n","cocoGt=COCO(annFile)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CNyeqbngdIDj","colab_type":"code","colab":{}},"source":["#initialize COCO detections api\n","resFile='%s/results/%s_%s_fake%s100_results.json'\n","resFile = resFile%(dataDir, prefix, dataType, annType)\n","cocoDt=cocoGt.loadRes(resFile)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ug9uwJr9dLQt","colab_type":"code","colab":{}},"source":["imgIds=sorted(cocoGt.getImgIds())\n","imgIds=imgIds[0:100]\n","imgId = imgIds[np.random.randint(100)]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"O-h6009SdNf3","colab_type":"code","colab":{}},"source":["# running evaluation\n","cocoEval = COCOeval(cocoGt,cocoDt,annType)\n","cocoEval.params.imgIds  = imgIds\n","cocoEval.evaluate()\n","cocoEval.accumulate()\n","cocoEval.summarize()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nkv4_aJCdP6A","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fceNyzh4lk8C","colab_type":"text"},"source":["Random"]},{"cell_type":"code","metadata":{"id":"CysOsHzoKnm7","colab_type":"code","colab":{}},"source":["from keras.applications.vgg16 import VGG16\n","import keras as k\n","\n","model = VGG16(weights='imagenet',\n","              input_shape=(224,224,3), \n","              include_top=True)\n","\n","print(model.summary())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XoHQedHbl5GO","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N8HTjKITl5xv","colab_type":"text"},"source":["# Faster RCNN"]},{"cell_type":"code","metadata":{"id":"qFgSdzTlHLaP","colab_type":"code","colab":{}},"source":["import numpy as np\n","import keras as k\n","\n","class AnchorProposals:\n","  def __init__(self,\n","               W,\n","               H,\n","               input_shape,\n","               ratios,\n","               resolutions,\n","               mini_batch_size):\n","    \"\"\"\n","    variables:\n","      W = output convolutional width\n","      H = output convolutional width\n","      input_shape\n","      model = a keras applications model\n","      input_shape = shape of the input image\n","      ratios = the ratios to use for the anchors\n","      resolutions = the resolutions to use for the anchors\n","      mini_batch_size = the mini_batch_size to be returned by generator\n","    \"\"\"\n","    self.W = W\n","    self.H = H\n","    self.input_shape = model.input_shape[1:3]\n","    self.ratios = ratios\n","    self.resolutions = resolutions\n","    if mini_batch_size % 2 != 0:\n","      raise ValueError(\"mini_batch_size needs to be an even number (% 2 == 0)\")\n","    self.mini_batch_size = mini_batch_size\n","    self.anchors = self.calc_anchor_pos()\n","    self.anchor_boxes = np.array([self.calc_anchor_box(self.anchors[it,:])\n","                                  for it in range(self.anchors.shape[0])])\n","  \n","  def calc_anchor_pos(self):\n","    \"\"\"pre-generates the list of all possible anchors\n","    This functions runs only once.\n","    returns:\n","      list of all anchors [W,H,position in ratio array, resolution]\n","    \"\"\"\n","    # calculate the anchor positions on the original image from the size\n","    # of the last convolutional feature map\n","    Ws = np.arange(0, self.input_shape[0], int(self.input_shape[0]/self.W))\n","    Hs = np.arange(0, self.input_shape[1], int(self.input_shape[1]/self.H))\n","    \n","    # broadcast each of the anchor variable arrays to combined size\n","    # of [len Ws, len Hs, len ratios, len resolutions]\n","    a = np.meshgrid(Ws,Hs,np.arange(len(self.ratios)),self.resolutions)\n","    \n","    # stack the anchor variable arrays to a combined array\n","    # of size [4, len Ws, len Hs, len ratios, len resolutions]\n","    a = np.stack(a)\n","    \n","    # reshape the combined array to a 2D array with [4,N]\n","    a = a.reshape((a.shape[0],np.prod(a.shape[1:])))\n","    \n","    # transpose to desired [N,4] and return\n","    a = a.T\n","    return a\n","  \n","  def calc_anchor_box(self, anchor, minmax=False):\n","    \"\"\"from the anchors calculate the proper box coordinates\n","    \n","    \"\"\"\n","    width = anchor[3] * self.ratios[anchor[2]][0]\n","    height = anchor[3] * self.ratios[anchor[2]][1]\n","    if minmax:\n","      x = anchor[0] - int(width/2)\n","      y = anchor[1] - int(height/2)\n","    else:\n","      x = anchor[0]\n","      y = anchor[1]\n","      \n","    return x, y, width, height\n","  \n","  # TODO maybe move this to the C++ function from pycocotools\n","  @staticmethod\n","  def IoU(box1, box2):\n","    \"\"\"intersect over union calculator between bounding box 1 & 2\n","    variable:\n","      a = [x_center, y_center, width, height]\n","      b = [x_center, y_center, width, height]\n","    \"\"\"\n","    # determine the (x, y)-coordinates of the intersection rectangle\n","    x1 = max(box1[0] - int(box1[2]/2),\n","             box2[0] - int(box2[2]/2)) \n","    y1 = max(box1[1] - int(box1[3]/2),\n","             box2[1] - int(box2[3]/2)) \n","    x2 = min(box1[0] + int(box1[2]/2),\n","             box2[0] + int(box2[2]/2)) \n","    y2 = min(box1[1] + int(box1[3]/2),\n","             box2[1] + int(box2[3]/2))\n","\n","    # compute the area of intersection rectangle\n","    interArea = max(0, x2 - x1) * max(0, y2 - y1)\n","\n","    # compute the area of both the prediction and ground-truth\n","    # rectangles\n","    box1Area = box1[2] * box1[3]\n","    box2Area = box2[2] * box2[3]\n","\n","    # compute the intersection over union by taking the intersection\n","    # area and dividing it by the sum of prediction + ground-truth\n","    # areas - the interesection area\n","    iou = interArea / float(box1Area + box2Area - interArea)\n","\n","    # return the intersection over union value\n","    return iou\n","    \n","  # TODO: maybe allow parallel or vectorized for this\n","  # TODO: maybe shuffle async after yield\n","  # TODO: we probably need converters between the different box styles\n","  # TODO: rewrite this with the followig order per image:\n","  #       1.) calc iou for all anchors in vectorized form\n","  #       2.) do non-maximum supression on anchors per gt\n","  #       3.) sample a even mini_batch by rounds per gt\n","  # TODO: anchor proposals during training vs during inference ???\n","  #       how does it work?\n","  def generator(self, gts, parallel=False, lazy=True):\n","    \"\"\"generates mini_batches with if possible half negative half positive\n","    samples, if not enough possitive, pads with negative samples.\n","    negative == IoU < 0.3 | positive == IoU > 0.7\n","    \n","    variables:\n","      gts: list of the ground truth boxes [[x_min, y_min, width, height], ...]\n","      parallel: creates mini_batch in parallel, only use if not\n","                one generator per thread\n","      lazy: only searches for positive and negative sample until\n","            mini_batch_size is reached\n","    \"\"\"\n","    # the max number of neg & pos elements in a perfect mini_batch\n","    b_max = int(self.mini_batch_size / 2)\n","    \n","    while True:\n","      # shuffle list of anchors to make sure we recieve different ones every time\n","      np.random.shuffle(self.anchors)\n","\n","      # count the number of neg & pos samples\n","      neg_cnt = 0\n","      pos_cnt = 0\n","      \n","      # preallocate a mini_batch list\n","      a_boxes = np.zeros((self.mini_batch_size, 4), dtype=np.float16)\n","      gts_boxes = np.zeros((self.mini_batch_size, 4), dtype=np.float16)\n","      gts_cls = np.zeros((self.mini_batch_size,), dtype=np.float16)\n","      # iterate through all anchor_boxes\n","      for it in range(self.anchor_boxes.shape[0]):\n","        \n","        # extract the anchor box\n","        a_box = self.anchor_boxes[it, :]\n","        \n","        # for every ground trough calculate the iou and\n","        # find the max iou for the anchor\n","        ious = [self.IoU(a_box, gt) for gt in gts]\n","        iou_max = max(ious)\n","        gt_max = gts[ious.index(iou_max)]\n","\n","        # positive samples are iou > 0.7, negative samples iou < 0.3\n","        # continue to add samples as long as there are not enough pos or neg\n","        # prefill the array if not completely full with the more abundant one\n","        #TODO: if performance is to low add the anchor with highest IoU for a gt as positive\n","        if (iou_max > 0.7 and \n","            (pos_cnt < b_max or pos_cnt + neg_cnt < self.mini_batch_size)):\n","          \n","          # add positive samples from the left\n","          a_boxes[pos_cnt,:] = a_box\n","          gts_boxes[pos_cnt,:] = gt_max\n","          gts_cls[pos_cnt] = 1.0\n","          \n","          # increase positives counter\n","          pos_cnt += 1\n","        \n","        elif (iou_max < 0.3 and \n","              (neg_cnt < b_max or pos_cnt + neg_cnt < self.mini_batch_size)):\n","          \n","          #increase negatives counter\n","          neg_cnt += 1\n","\n","          # add negative samples from the right\n","          a_boxes[-1 * pos_cnt,:] = a_box\n","          gts_boxes[-1 * pos_cnt,:] = [0,0,0,0]\n","          gts_cls[-1 * pos_cnt] = 0.0\n","          \n","        elif pos_cnt >= b_max and neg_cnt >= b_max:\n","          break\n","      \n","      yield a_boxes, gts"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8i6IjsrbOXM1","colab_type":"code","colab":{}},"source":["from keras.engine.base_layer import Layer\n","\n","class ROIPooling(Layer):\n","  def __init__(self,\n","               mini_batch_size,\n","               W = 7,\n","               H = 7,\n","               strides=None,\n","               padding='valid',\n","               data_format=None,\n","               **kwargs):\n","    super(ROIPooling, self).__init__(name=name, **kwargs)\n","  \n","    self.mini_batch_size = mini_batch_size\n","    self.W = W\n","    self.H = H\n","    self.strides = strides\n","    self.padding = padding\n","    self.data_format = data_format\n","    self.built = True\n","  \n","  def build(input_shape):\n","    self.built = True\n","  \n","  def call(self, inputs)\n","    assert isinstance(inputs, list)\n","    \n","    x = inputs[0]\n","    rois = inputs[1]\n","    \n","    if rois.shape[0] / self.mini_batch_size < x.shape[0]:\n","      raise TypeError(\"batch size, num ROIs and mini-batch size missmatch, \" +  \n","                      \"there must be ROIs({}) /\".format(rois.shape[0]) +\n","                      \"mini_batch_size({}) = \".format(self.mini_batch_size) + \n","                      \"batches({}) \".format(rois.shape[0] / self.mini_batch_size) +\n","                      \"in the inputs, but recieved {} as batch size\".format(x.shape[0]))\n","    \n","    outputs = k.backend.placeholder(self.compute_output_shape(x.shape),\n","                                    dtype=np.float16,\n","                                    name=\"ROI_pool_output\")\n","    \n","    #TODO check if this loop can be unroled and vectorized\n","    for roi_it in range(rois.shape[0])\n","    \n","      roi = rois[roi_it]\n","    \n","      pool_w = roi.shape[2] / self.W\n","      pool_h = roi.shape[3] / self.H\n","\n","      if self.data_format == 'channels_last':\n","        pool_shape = (1, pool_w, pool_h, 1)\n","        strides = (1,) + self.strides + (1,)\n","      else:\n","        pool_shape = (1, 1, pool_w, pool_h)\n","        strides = (1, 1) + self.strides\n","      \n","      outputs[roi_it, :] = nn.max_pool(\n","          x[int(roi_it/self.mini_batch_size),\n","            roi[0]:roi[0]+roi[2],\n","            roi[1]:roi[1]+roi[3],\n","            :]\n","          ksize=pool_shape,\n","          strides=strides,\n","          padding=self.padding.upper())\n","      \n","      return outputs\n","    \n","  def compute_output_shape(self, input_shape):\n","    assert isinstance(input_shape, list)\n","    return input_shape[1][0] * self.mini_batch_size + [self.W, self.H] + input_shape[0][3]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ScZw1S_cDHUj","colab_type":"code","colab":{}},"source":["class RPN(Layer):\n","    def __init__(self,\n","                 feature_vector_length = 256,\n","                 conv_filter_size = 3,\n","                 num_anchors = 128)\n","\n","      self.feature_vector_length = feature_vector_length\n","      self.conv_filter_size = conv_filter_size\n","      self.num_anchors = num_anchors\n","\n","    def build(input_shape):\n","      self.built = True\n","      \n","    def call(self, x):\n","      #TODO: maybe regularization\n","      x = k.layers.Conv2D(self.feature_vector_length,\n","                          (self.conv_filter_size, self.conv_filter_size),\n","                          activation='relu',\n","                          kernel_initializer='normal',\n","                          name=\"RPN_base_conv\")(x)\n","      \n","      x_reg = k.layers.Conv2D(4 * self.num_anchors,\n","                              (1, 1),\n","                              activation='linear',\n","                              kernel_initializer='normal',\n","                              name=\"RPN_reg\")(x)\n","      \n","      x_cls = k.layers.Conv2D(self.num_anchors,\n","                              (1, 1),\n","                              activation='sigmoid',\n","                              kernel_initializer='normal',\n","                              name=\"RPN_cls\")(x)\n","      \n","      return x_reg, x_cls\n","    \n","    def compute_output_shape(self, _):\n","      return [4 * self.num_anchors, self.num_anchors]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jGOs_df9DKqV","colab_type":"code","colab":{}},"source":["def RPN2ROI(x_reg, x_cls):\n","    treshold = 0.5\n","    mask = Lambda(lambda x: backend.greater(x, treshold))(x_cls)\n","    rois = Lambda(lambda x: tf.boolean_mask(x, mask))(x_reg)\n","    return rois\n","    \n","#TODO We still need to go from ROI to last convolutional layer level"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jCEgwbR3EJIs","colab_type":"code","colab":{}},"source":["# TODO: gibt es bei Keras eine loss base class die verwendet werden kann?\n","def smooth_L1_loss(x)\n","    \"\"\" calculate smooth L1 loss according to Ross Girshick, 2015, Fast-RCNN\n","    per box\n","\n","    variable:\n","      x = t_predicted - t_groundtruth; tensor with shape (num_predictions, 4)\n","    returns:\n","      the loss per box as a tensor with shape (num_predictions)\n","\n","    smooth L1 loss:\n","            | 0.5 * x^2  if |x| < 1\n","    l(x) =  |\n","            | |x| - 0.5  else\n","    \"\"\"\n","    # placeholder for loss elements\n","    l_elem = k.backend.placeholder(x.shape, dtype=np.float16)\n","\n","    # index to access the |x| < 1 values\n","    ind = [!k.backend.less(k.backend.abs(x), 1)]\n","\n","    # calculate loss parts if |x| < 1 first and sum them up over x,y,w,h\n","    l_elem[ind] = 0.5 * k.backend.pow(x[ind], 2)\n","\n","    # add sum of loss parts for |x| > 1 \n","    l_elem[!ind] = k.backend.abs(x[!ind]) - 0.5\n","\n","    # sum over x,y,w,h for loss per box\n","    l = k.backend.sum(l_elem, axis = 1)\n","\n","    #return loss\n","    return l\n","\n","\n","class MultiTaskLoss:\n","  \"\"\"calculate multitask loss for rpn\n","  \n","  keras does not allow\n","\n","  variables:\n","    gts_reg: groundtruth box coordinates  \n","\n","\n","  \"\"\"\n","  def __init__(self, pos_thresh = 0.5, lam = 1.0, **kwargs)\n","    self.pos_thresh = pos_thresh\n","    self.lam = lam\n","    \n","  #(gts_reg, a_reg, x_reg, gts_cls, x_cls, ):\n","  def __call__(y_true, y_pred):\n","    \"\"\"\n","    calculate the Faster RCNN Multitask loss for object detection, \n","    Classification Accuracy & Position Regression Accuracy\n","    \n","    variables:\n","      y_true = [gts_reg, gts_anchors, gts_cls]\n","      gts_reg = groundtruth regression coordinates [x, y, width, height]\n","      gts_anchors = per groundtruth regression coordinates the corresponding anchor [x, y, width, height]\n","      gts_cls = groundtruth class\n","      y_pred = [y_reg, y_anchors, y_cls]\n","      y_reg = predicted regression coordinates [x, y, width, height]\n","      y_anchors = per predicted regression coordinates the corresponding anchor [x, y, width, height]\n","      y_cls = predicted class\n","\n","    t_coordinates:\n","      t_x = (x - a_x) / a_width\n","      t_y = (y - a_y) / a_height\n","      t_width = log(width / a_widht)\n","      t_height = log(height / a_height)\n","      \n","    returns:\n","      multitask loss according to Girschick - Fast-RCNN, 2015 \n","    \"\"\"\n","    # transform the predicted box coordinates to the t coordinates\n","    y_reg, y_a, y_cls = y_pred\n","    t = k.backend.placeholder(gts_reg.shape, dtype=np.float16)\n","    t[:,0] = (y_reg[:,0] - y_a[:,0]) / y_a[:,2]\n","    t[:,1] = (y_reg[:,1] - y_a[:,1]) / y_a[:,3]\n","    t[:,2] = k.backend.log(y_reg[:,2] / y_a[:,2])\n","    t[:,3] = k.backend.log(y_reg[:,3] / y_a[:,3])\n","    \n","    #transform the groundtruth box coordinaties to the t coordinates\n","    gts_reg, gts_a, gts_cls = y_true\n","    ts = k.backend.placeholder(gts_reg.shape, dtype=np.float16)\n","    ts[:,0] = (gts_reg[:,0] - gts_a[:,0]) / gts_a[:,2]\n","    ts[:,1] = (gts_reg[:,1] - gts_a[:,1]) / gts_a[:,3]\n","    ts[:,2] = k.backend.log(gts_reg[:,2] / gts_a[:,2])\n","    ts[:,3] = k.backend.log(gts_reg[:,3] / gts_a[:,3])\n","\n","    # calculate the regression part of the loss as smooth L1 loss\n","    #TODO this should be 0 if background else 1\n","    l_reg = k.layers.multiply(gts_cls, self.smooth_L1_loss(t - ts))\n","\n","    # turn the class predictions from confidence into catigorical\n","    #TODO this is not categorical if more than two classes\n","    x_cls_cat = k.backend.round(y_cls - (pos_thresh - 0.5))\n","\n","    # calculate the catigorical cross_entropy / logloss\n","    l_cls = k.losses.categorical_crossentropy(gts_cls, x_cls_cat)\n","\n","    # calculate the combined loss\n","    l_rpn = l_cls / gts_cls.shape[0] + lam * l_reg / gts_reg.shape[0]\n","\n","    return l_rpn\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cMq8d2alreFY","colab_type":"code","outputId":"4d228ef6-dd82-4669-8812-bbce8e06f5ed","executionInfo":{"status":"error","timestamp":1563208689999,"user_tz":-120,"elapsed":1145,"user":{"displayName":"Tobias Grundmann","photoUrl":"","userId":"08188606845298769954"}},"colab":{"base_uri":"https://localhost:8080/","height":132}},"source":["from keras.model import Model\n","\n","#TODO create opportunity to load weights from xxx dataset\n","def FasterRCNN(base_model,\n","               num_classes,\n","               feature_vector_length = 256,\n","               num_anchors = 128,\n","              ):\n","    \"\"\"\n","    \n","    variables:\n","      base_model: a keras applications image classification model\n","      num_classes: the number of classes to be detected\n","      feature_vector_length: the feature vector size before prediction\n","      num_anchors: max number of anchors to propose\n","    \n","    \"\"\"\n","  \n","    #TODO extract from the model the right layer, i.e check if -1 is a max_pool than it should be the -2 layer\n","    x = base_model.output\n","    \n","    #rpn\n","    rpn = RPN(feature_vector_length,\n","              num_anchors)\n","    x_reg, x_cls = rpn(x)\n","    \n","    #create a rpn model, since we need to train on the rpn level as well\n","    rpn_model = models.Model(base_model.inputs, [x_cls, x_reg], name='RPN')\n","    \n","    #turn selected rpn into rois\n","    rois = RPN2ROI(x_reg, x_cls)\n","    \n","    #roipooling\n","    roipooling = ROIPooling(mini_batch_size)\n","    x = roipooling([x, rois])\n","    \n","    #detection block of the model\n","    x = l.layers.Flatten()(x)\n","    #TODO: the SVD stuff from the Fast RCNN paper\n","    #TODO: what is the proper size for this\n","    #TODO: this has to not only accept batch but as well num rois like (batch, num_rois, ...)\n","    x = k.layers.Dense(4096,\n","                       activation=None,\n","                       name = \"Detection_bloc_dense_1\")(x)\n","    x = k.layers.Dense(feature_vector_length,\n","                       activation=None,\n","                       name = \"Detection_bloc_dense_2\")(x)\n","    x_cls = k.layers.Dense(num_classes + 1,\n","                           activation=\"softmax\",\n","                           name = \"Detection_bloc_cls\")\n","    x_reg = k.layers.Dense(4,\n","                           activation=\"linear\",\n","                           name = \"Detection_bloc_reg\")\n","    \n","    # create the complete model\n","    model = models.Model(base_model.inputs, [x_cls, x_reg], name='Faster-RCNN')\n","    \n","    return model, rpn_model, base_model"],"execution_count":0,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-82fe78a6c7ec>\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    def RPN(self, x, num_anchors)\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"code","metadata":{"id":"FUsowC9U4SWY","colab_type":"code","colab":{}},"source":["def RPN_data_generator(data, W, H, input_shape, ratios, resolutions, mini_batch_size):\n","  \"\"\"training data generator for the RPN\n","  takes training data as images and boxes\n","  and extracts anchorProposals as groundtruth\n","  from those.\n","  \n","  variables:\n","    data: input data ??? as tfrecord i guess, what else could it be ???\n","    **kwargs: variables for anchorProposals\n","  \"\"\"\n","  \n","  anchorProposals = AnchorProposals(W=W,\n","                                    H=H,\n","                                    input_shape,\n","                                    ratios,\n","                                    resolutions,\n","                                    mini_batch_size\n","                                   )\n","  \n","  while True:\n","    #extract one batch of images with bounding boxes\n","    \n","    \n","    #extract the minibatch of anchors for each image\n","    anchorProposals.generator(gts=)\n","    \n","    #yield a batch of minibatches \n","    #(if there is more than one image, make the batch dimensions batch_size * mini_batch_size\n","    yield True"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dM9pfvh48juR","colab_type":"code","colab":{}},"source":["def ObjectDetection_data_generator(data):\n","  \n","  \n","  while True:\n","    #extract one batch of images with bounding boxes\n","    \n","    \n","    #yield a batch of images with bounding boxes\n","    yield True"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"d1MWIdL7bg9L","colab_type":"code","colab":{}},"source":["def train_FasterRCNN(base_model,\n","                     model_kwargs,\n","                     rpn_training_kwargs,\n","                     detection_training_kwargs,\n","                     data,\n","                     train_from_detection = False,\n","                     detection_weights = None,\n","                     epochs = 100,\n","                     steps_rpn = 100,\n","                     steps_detection = 100,\n","                     ratios = [(1,1),(1,2),(2,1)],\n","                     resolutions = [128,256,512],\n","                     mini_batch_size = 256,\n","                     **kwargs\n","                    ):\n","  \"\"\"\n","  Ren et al - Faster RCNN - 2017:\n","  In the first step, we train the RPN as described above. This network is\n","  initialized with an ImageNet- pre-trained model and fine-tuned end-to-end\n","  for the region proposal task. In the second step, we train a separate\n","  detection network by Fast R-CNN using the proposals generated by the step-1\n","  RPN. This detection network is also initialized by the ImageNet-pre-trained\n","  model. At this point the two networks do not share conv layers. In the\n","  third step, we use the detector network to initialize RPN training, but we\n","  fix the shared conv layers and only fine-tune the layers unique to RPN. Now\n","  the two networks share conv layers. Finally, keeping the shared conv layers\n","  fixed, we fine-tune the fc layers of the Fast R-CNN. As such, both networks\n","  share the same conv layers and form a unified network.\n","  \n","  variables:\n","    model: the Faster-RCNN model\n","    model_rpn: the RPN model\n","    base_model: the image classification model\n","    train_from_detection: whether to use object detection or image classification\n","        weights for finetuning\n","    detection_weights: The type of detection weights if train_from_detection is \n","        True. Any of [\"Pascal\",\"COCO\",\"OID\"]\n","    metrics: keras callback for metrics to use\n","    ratios: ratios for anchorproposals\n","    resolutions: resolutions for anchorproposals\n","    mini_batch_size: mini_batch_size for anchorproposals\n","    **kwargs: keras.model.fit_generator variables after epochs\n","  \"\"\"\n","  # Step 1: a image_net based rpn\n","  gen_RPN = RPN_data_generator(data=data,\n","                               W=int(base_model.layers[-2].output.shape[1]),\n","                               H=int(base_model.layers[-2].output.shape[2]),\n","                               input_shape=base_model.input.shape,\n","                               ratios=ratios,\n","                               resolutions=resolutions,\n","                               mini_batch_size=mini_batch_size)\n","  \n","  _, model_rpn_tmp =  FasterRCNN(base_model,\n","                             num_classes,\n","                             **model_kwargs)\n","  \n","  model_rpn_tmp.fit_generator(\n","              generator = gen_RPN,\n","              steps_per_epoch=steps_rpn,\n","              epochs=epochs,\n","              **rpn_training_kwargs\n","          )\n","  \n","  #--> we need to get the ROIS from this model an funnel them into the ROIS layer in model\n","  \n","  #TODO ensure this is a deep copy not a shared one\n","  # Step 2: a image_net based detection with model_rpn ROI\n","  gen = ObjectDetection_data_generator(data=data)\n","  \n","  model, model_rpn =  FasterRCNN(base_model,\n","                         num_classes,\n","                         **model_kwargs)\n","  \n","  model.fit_generator(\n","              generator = gen,\n","              steps_per_epoch=steps_detection,\n","              epochs=epochs,\n","              **detection_training_kwargs\n","          )\n","  \n","  #TODO are Step 3 & 4 sequential or iterative per epoch??\n","  # Step 3: keep model from step 2, finetune rpn layer\n","  gen_RPN = RPN_data_generator(data=data,\n","                               W=int(base_model.layers[-2].output.shape[1]),\n","                               H=int(base_model.layers[-2].output.shape[2]),\n","                               input_shape=base_model.input.shape,\n","                               ratios=ratios,\n","                               resolutions=resolutions,\n","                               mini_batch_size=mini_batch_size)\n","  \n","  #TODO freeze the base model layers\n","  model_rpn_tmp.fit_generator(\n","              generator = gen_RPN,\n","              steps_per_epoch=steps_rpn,\n","              epochs=epochs,\n","              **rpn_training_kwargs\n","          )\n","  \n","  # Step 4: keep model from step 3, finetune detection layer\n","  gen = ObjectDetection_data_generator(data=data)\n","  \n","  #TODO make sure base_model & rpn layers are frozen\n","  model.fit_generator(\n","              generator = gen,\n","              steps_per_epoch=steps_detection,\n","              epochs=epochs,\n","              **detection_training_kwargs\n","          )\n","  \n","  \n","  return model\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yYGXtImbbeig","colab_type":"text"},"source":["# future"]},{"cell_type":"code","metadata":{"id":"AXQylXqhn8jQ","colab_type":"code","colab":{}},"source":["def main(_):\n","\n","  resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=FLAGS.tpu)\n","  tf.tpu.experimental.initialize_tpu_system(resolver)\n","  strategy = tf.distribute.Strategy(resolver)\n","\n","  data = SyntheticDataset(FLAGS.batch_size)\n","  \n","  with strategy.scope():\n","    model = model_cls(weights=None, input_shape=data.input_shape,\n","                      classes=data.num_classes)\n","\n","    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n","    model.compile(loss=\"categorical_crossentropy\",\n","                  optimizer=optimizer,\n","                  metrics=[\"accuracy\"])\n","\n","    history = model.fit(\n","        data.train_dataset,\n","        epochs=FLAGS.epochs,\n","        steps_per_epoch=data.num_train_images // FLAGS.batch_size,\n","        validation_data=data.test_dataset,\n","        validation_steps=data.num_test_images // FLAGS.batch_size)\n","\n","    return history.history"],"execution_count":0,"outputs":[]}]}